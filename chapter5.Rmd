# 5: Dimensionality reduction techniques

**Part 1**

The wrangled dataset consists of 8 columns describing quality of life factors by country. These variables are:

- "Life"            (num) Life expectancy at birth
- "Edu"             (num) Expected years of schooling
- "GNI"             (chr) Gross National Income per capita
- "Maternal_mort"   (int) Maternal mortality ratio
- "Adol_birth"      (num) Adolescent birth rate
- "Representation"  (num) Percent of representation in parliament
- "Edu_ratio"       (num) Edu_F / Edu_M ratio
- "Labour_ratio"    (num) Labour_F / Labour_M ratio

Let's explore the data and plot out the possible correlations!

```{r}
# Libraries
library(dplyr); library(GGally);library(corrplot)

# Fetching the wrangled data
human <- read.table("data/human.txt")

glimpse(human)

ggpairs(human)

cor(human) %>% corrplot (type = "lower", tl.cex = 0.7, tl.pos="d", cl.pos="r")

ggplot(human, aes(x = Edu, y = Life)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Education equals life?")

ggplot(human, aes(x = Maternal_mort, y = Life)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Maternal mortality does not equal life.")

```
\
The highest positive correlations seem to be between education and life expectancy, and between maternal mortality and adolescent bith rate. Strongest negative correlation is between life expectancy and maternal mortality, which kind of makes sense. From the ggpairs we see how the data points plot against each other - the highest positive and negative correlation pairs seem like good candidates for fitting some nice linear regression lines to them, so I did. I did not bother to analyze these plots or the fitted lines' intercepts any further, though - this exercise is, after all, about multivariate analysis.

**Part 2**

First, we perform PCA (principal component analysis) to the unaltered data:

```{r, message=FALSE, warning=FALSE}

# PCA (with the SVD method)
pca_human1 <- prcomp(human)
summary(pca_human1)

# Biplot of the PC representation and the original variables
biplot(pca_human1, choices = 1:2, cex = c(0.5, 0.7), col = c("grey80", "deeppink2"))

```
\
The variable GNI completely dominates and explains all the variation in the raw data, as the numerical values of GNI are so huge in comparison to the other variables. Thus, we need to standardize the data to get more meaningful results from this analysis.

**Part 3**

Let's scale the data, perform the analysis again, and name our biplot labels according to the variance percentages!

```{r, message=FALSE, warning=FALSE}
# Scaling the data and performing the PCA analysis again
human_s <- scale(human)
pca_human <- prcomp(human_s)

summary(pca_human)

# Calculate and print rounded percentages of variance
s <- summary(pca_human)
pca_p <- round(1*s$importance[2, ]*100, digits = 1)
pca_p

# Assign corresponding percentages to labels
pc_l <- paste0(names(pca_p), " (", pca_p, " %)")

# Scaled biplot with corresponding variance percentages in labels
biplot(pca_human, choices = 1:2, cex = c(0.5, 0.7), col = c("grey80", "deeppink2"), xlab = pc_l[1], ylab = pc_l[2])

```
\
Now the data is much more readable. 


**Part 4**

We can see that the principal component 1 explains up to 53,6 % of the variation, and it consists of variables such as life expectancy and educational factors. Gender factors, such as labour ratio and representation in the parliament, explain 16,2 % of the variation. Education, life expectancy and GNI cluster to the far left of the biplot, and maternal mortality and adolescent birth ratio to the far right, and from the arrows we can see that they have opposing effects.


**Part 5**

Here we explore data of tea drinking habits from FactoMineR library. The original dataset contains 36 different variables (see colnames below for the full list), but I decided to separate the data containing information about tea drinking places, the type of tea (earl grey, green or black) and age quartales (the variable age_G) for a closer look. We visualize these selected variables and their internal distribution by simple column charts, and then perform Multiple Correspondence Analysis (MCA) to see if any patterns can be interpreted. 

```{r, message=FALSE, warning=FALSE}
library(FactoMineR); library(tidyr)
data("tea")
colnames(tea)

teaplaces <- tea[,c(7:13,23)]

colnames(teaplaces)

gather(teaplaces) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))

mca <- MCA(teaplaces, graph = F)
summary(mca)

plot(mca, invisible=c("ind"), habillage = "quali")
```
\
The first dimension explains 15.06 % of the variance in this data subset, and adding Dim 2, the cumulative explanatory value ends up to 26.76 %. This is not very high explanatory value, but we can see some interesting patterns from the visualization nevertheless. Drinking green tea clusters to the far left and together with "not somewhere" values - home being the closest place variable to it. Earl Grey is most likely drank by young people, and not at home. Black tea is favored by the older folks. Out of this test group, the 35-44 year old age quartile is the most likely to drink tea in a tearoom or restaurant. They will also most likely drink black tea, but this preference is not as strong as within the oldest group. From the previous column charts we saw that the green tea drinkers, as well as those who drank tea somewhere else than "home", were in the clear minority, which explains why these groups are so distinguished in the MCA biplot.
