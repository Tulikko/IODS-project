# 4: Clustering and classification
## Part 1

In this exercise we created ‘chapter4.Rmd’ to perform the clustering and classification analysis in.

## Part 2

Here I initialize the necessary libraries and load the analysis data from MASS library. The dataset consists of 14 variables with 506 observations about housing values in suburbs of Boston.

```{r, message=FALSE, warning=FALSE}
# Libraries
library(MASS)
library(corrplot)
library(dplyr)
library(ggplot2)
library(GGally)

# Load the Boston data from MASS:
data("Boston")

```

The variables are **crim** (crime rate), **zn** (residential land), **indus** (non-retail business acres), **chas** (dummy variable), **nox** (nitrogen oxides concentration in ppm), **rm** (average rooms), **age** (built prior to 1940), **dis** (distances to employment centres), **rad** (accessibility to radial highways), **tax** (tax rate), **ptratio** (pupil-teacher ratio), **black** (relative proportion of blacks), **lstat** (lower status %) and **medv** (owner-occupied homes).

```{r}
# Exploring the data:
dim(Boston)
str(Boston)

```

## Part 3

Here we explore the dataset a bit further by showing summary of the variables, and then exploring correlations between variables both numerically and visually.

```{r}
# Summary of the variables
summary(Boston)
```
Like we can see from the min/max/mean values, while all the data are numerical, the scale is very wide, ranging from sub-zero values to over seven hundred. Clustering values with such differences in scale can prove to be challenging. However, let's take a look at correlations between variables first! 

```{r}

# Calculating correlation matrix and rounding it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# Printing the matrix
cor_matrix

# Visualizing the matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

```
The correlation plot sure is pretty, and it seems like strongest correlation with our variable of interest, crime rate (crim), is with "tax" and "rad" variables.


## Part 4

In this part we standardize the dataset to have more comparable values. If one variable has huge values with big differences, and other has intrinsically very small values with equally small differences, we might miss correlations between them as the bigger scale completely dominates the output. 

There are different ways to rescale or "normalize" data for analysis. Here we use the most common way, the "z-score", which represents the amount of standard deviations by which the observation differs from the mean value. It can have positive or negative value. Point is, when all data is rescaled like this, we get comparable results for clustering.

$$scaled(x)=\frac{x-mean(x)}{sd(x)}$$

```{r}
# Scaling the data
boston_scaled <- scale(Boston)

# Changing the class to data frame from "matrix" "array"
boston_scaled <- as.data.frame(boston_scaled)

# Summary of the scaled variables
summary(boston_scaled)

```
The values are much more comparable now. Let's create a categorical variable of the crime rate ("crim"). We use the quantiles as break points in the categorical variable (0 %/25 %/50 %/75 %/100 %) and drop the old crime rate variable. Finally, we divide the dataset to "train" and "test" sets, with 80% of the data in the "train" set.

```{r}
# Quantile vector of variable 'crim':
bins <- quantile(boston_scaled$crim)
bins

# Categorical variable 'crime':
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high","high"))

# Removing original 'crim'
boston_scaled <- dplyr::select(boston_scaled, -crim)

# Adding 'crime'
boston_scaled <- data.frame(boston_scaled, crime)

# Choosing randomly 80% of the rows:
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)

# Dividing to train and test sets:
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

## Part 5

Here we perform the linear discriminant analysis of how the crime rate classes (broken into "low","med_low","med_high" and "high" in the previous part) cluster, and visualize results in a biplot. 

```{r}
# Linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
lda.fit

# Function for LDA biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# Target classes as numeric
classes <- as.numeric(train$crime)

# Plot the LDA results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```

If I understand the plot correctly, variable "rad" (accessibility to radial highways) is very strong indicator of high (blue) crime rate values. High "zn" (residential land) value is somewhat indicative of low (black) values, and "nox" (nitrogen oxides) correlates most to med_high crime rate values.

## Part 6

In this part we predict the classes with the LDA model on the test data, and cross tabulate the results with the crime categories from the "test" set. 

In the cross tabulation we can see that the model was excellent in predicting high crime rate values (zero mispredicted results), but lower crime rates have a bit more erroneus results, even if the clear majority still was predicted correctly.

```{r}
# Save the correct classes from test data:
correct_classes <- test$crime

# Remove the crime variable from test data:
test <- dplyr::select(test, -crime)

# Predict classes with test data:
lda.pred <- predict(lda.fit, newdata = test)

# Cross-tabulate the results:
table(correct = correct_classes, predicted = lda.pred$class)

```

## Part 7

Lastly, we reload the data and standardize it with the same method as in part 4. Now we calculate the distances between standardized observations, investigate what is the optimal number of clusters, and run k-means algorithm before and after determining the optimal amount of them.

The optimal amount of clusters is when the value of total Within Sum of Squares ("tWCSS") changes abruptly. With this data, the optimal number of clusters is 2.

From the two visualization plots (ggpairs) for two different K means cluster analysis results, with different number of clusters, we can easily see the difference. The latter one with only 2 clusters is visually much more convincing in its interpreted clusters than the one with 3 clusters.

```{r, message=FALSE, warning=FALSE}
# Reload data
data('Boston')

# Standardizing and changing class to data frame
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)

# Euclidean distance matrix
dist_eu <- dist(boston_scaled)
summary(dist_eu)

# Manhattan distance matrix
dist_man <- dist(Boston, method = 'manhattan')
summary(dist_man)

# Set seed to remove randomness between runs
set.seed(123)

# Maximum number of clusters
k_max <- 6

# K-means clustering, 1st attempt before determining optimal nr of clusters
km1 <-kmeans(Boston, centers = 3)

# Visualization by ggpairs, using different colours for different clusters 
cluster1 <- as.factor(km1$cluster)
ggpairs(Boston, mapping = aes(col = cluster1))

# Calculate the total within sum of squares to find the optimal number of clusters
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# Visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# K-means clustering
km2 <-kmeans(Boston, centers = 2)
summary(km2)

# Visualization by ggpairs, using different colours for different clusters 
cluster2 <- as.factor(km2$cluster)
ggpairs(Boston, mapping = aes(col = cluster2))

# Same with pairs function, not quite as pretty
pairs(Boston, col = cluster2)

```


